[#daytwooperations]
= 4. Day 2 Operations

At this point you have a _SNO_, with two ArgoCD instances deployed and _cluster-admin_ permissions managed by an _Argo Hub_. So you can start configuring day 2 operations.

As we might need to apply this configuration to multiple managed clusters, we need to be able to render multiple applications using Helm charts, even though we are applying this configuration
to our single scoped cluster.

Go to https://github.com/alvarolop/helm-infra-gitops-workshop[helm-infra-gitops-workshop] repository and checkout to your working branch.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/helm-infra-gitops-workshop    
git checkout sno-<clusterID>  
----  

Take a look to the code, this repo contains Helm charts for configuring day 2 operations in our managed cluster. It contains several dependencies and its _Charts_ with some default _values_ 
that you might override by creating your own _values_ file.

.helm-infra-gitops-workshop repository structure
image::helm-folder-1.png[]

At this point of the workshop, you will only have _values.yaml_. Try to render it, if you look at the _values.yaml_ file all the not _global_ fields are set to _false_, so you will be getting an empty output.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/helm-infra-gitops-workshop
helm template .
---- 

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cat values.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
oauth:
  enabled: false
bookinfo:
  enabled: false
operators: 
  enabled: false
monitoring: 
  enabled: false
namespace:
  enabled: false
nmstate: 
  enabled: false
app:
  enabled: false
vault:
  enabled: false  
global:
  render: true
  argocd:
    enabled: true
    controller: openshift-gitops #argocd
----

IMPORTANT: Each time you create a _values-example.yaml_ file in the following sections you can render it by specifying which _values_ file to use with *_-f_* flag.

E.g:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
helm template . -f values-example.yaml
---- 

You can apply Charts, first by creating a single Application that applies Charts _all-in-one_, or creating a single Application per Chart, or kind of configuration to be applied.

You will now deploy applications to our _SNO_ Argo console by hand, and then at the end of this section you will take a look at how these deployments can be automated.

- Log in *_argocd-infra_* instance using OpenShift credentials, with user _admin_ and <pass> provided by intructor. 

NOTE: Replace *_<clusterID>_* and *_<domain>_*. (E.g. https://argocd-infra-server-openshift-gitops.apps.sno-<clusterID>.<domain>)

- If you try to deploy this Application on your cluster you will see it _syncs_, but it does not render objects as every value is disabled.

IMPORTANT: Replace _<clusterID>_ and _<your_user>_ in the following Application.

- Click on: -> *_New app_* -> *_Edit as Yaml_*

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: day-2-sno-<clusterID>
  namespace: openshift-gitops
spec:
  destination:
    server: 'https://kubernetes.default.svc'
  project: default
  source:
    path: .
    repoURL: 'https://github.com/<your_user>/helm-infra-gitops-workshop.git'
    targetRevision: sno-<clusterID>
  syncPolicy:
    automated:
      prune: true
      selfHeal: true   
---- 

- Then hit *_Save_* -> *_Create_*

.Day 2 initial app with values disabled
image::day-2-sno-initial-app.png[]

This is because dependencies include conditions for _enabling/disabling_ those Charts. As every _value_ is set to _false_ it does not apply anything.

IMPORTANT: Once this subchapter is finished, delete the _day-2-sno-<clusterID>_ Application.

[#identityproviders]
== 4.1 Configure Identity Providers

As you can see, there is already a local Identity Provider configured but we want to override current users and integrate with other Identity Providers like _LDAP Free Ipa_ provider and _Keycloak_.

For configuring Identity Providers on OpenShift you just need to modify the existing _Oauth_ resource with the provider and its configuration data.

NOTE: Learn more about https://docs.openshift.com/container-platform/4.17/authentication/understanding-identity-provider.html[Configuring identity providers in OpenShift].

For some of them, we just need to set up the integration while for others like _LDAP_ apart from the integration against the server we also need to synchronize groups and users.

NOTE: _Keycloak_ and _LDAP Free IPA_ server are running on _Argo Hub_ cluster.

LetÂ´s deploy the _Oauth_ resources required and verify how groups are synchronized.

First we need to take a look to _charts/oauth/values.yaml_. As you can see, you need to define some missing _values_ for each managed cluster. You can do this by setting those on this Chart values file, setting them as parametes or defining a new _values.yaml file_ for your deployment. We will follow the second approach.

Go to your working branch in the https://github.com/alvarolop/helm-infra-gitops-workshop[helm-infra-gitops-workshop] repository and create a file called *values-oauth.yaml*:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/helm-infra-gitops-workshop
vi values-oauth.yaml
----

Insert the following configuration, replacing:

- *<clusterID> -> Assigned cluster*
- *<domain> -> Assigned domain cluster*
- *<ip> -> LDAP Server IP*
- *<nodeport> -> LDAP Server Port*
- *<your_keycloak_secret_data> -> See how to get this secret data after the code*

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
oauth:
  enabled: true # Enable dependency
  keycloak: # Update chart values
    clientid: myclient-<clusterID>
    issuer: https://keycloak-ingress-keycloak.apps.argo-hub.<domain>/realms/myrealm-<clusterID>
    data: <your_keycloak_secret_data>
  ldap:
    sync:
      ldap_url: 'ldap://<ip>:<nodeport>'
     
---- 

IMPORTANT: *How to get _Keycloack_ secret*: 

1) Login to _Argo Hub_ OpenShift console (E.g. https://console-openshift-console.apps.argo-hub.<domain>/)

- Hit *_Workshop Users_*. 

.Log in Argo Hub OpenShift console
image::hub-login-1.png[]

- Insert _userXX_ and _<pass>_ provided by instructor and click *_Log in_* button.
  
2) Find _Keycloack_ URL, in OpenShift _Routes_ section, it should be something like: https://keycloak-keycloak.apps.argo-hub.<domain>
  
3) Login in _Keycloack_ as _"admin/admin"_ (Administrative Console)

.Keycloack log in
image::keycloack-login.png[]

4) Navigate to your _realm-<clusterID>_

image::realm-navbar.png[]

5) Then go to your _myclient-<clusterID>_ 

6) Click on _Credentials_ tab, you will see your client secret (_<your_keycloak_secret_data>_) there.

Once the _values-oauth.yaml_ exists in your repository, push it and create a new Application in the _argocd-infra_ instance:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/helm-infra-gitops-workshop
git add .
git commit -m "identity providers"
git push origin sno-<clusterID>
----

Replace the following:

- *<clusterID> -> Assigned cluster*
- *<domain> -> Assigned domain cluster*
- *<your_user> -> Your GitHub user account*

Log in the _argocd-infra_ instance (E.g. https://argocd-infra-server-openshift-gitops.apps.sno-<clusterID>.<domain>)

To create _sno-<clusterID>-oauth_ Application, once logged in, click on:

- *_New app_* -> *_Edit as Yaml_* 

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: sno-<clusterID>-oauth
  namespace: openshift-gitops
spec:
  destination:
    server: 'https://kubernetes.default.svc'
  project: default
  source:
    helm:
      valueFiles:
        - values-oauth.yaml
    path: .
    repoURL: 'https://github.com/<your_user>/helm-infra-gitops-workshop.git'
    targetRevision: sno-<clusterID>
  syncPolicy:
    automated:
      prune: true
      selfHeal: true   
---- 

- Then hit *_Save_* -> *_Create_*

If you take a look to the Helm charts you will notice we do not only need to update _Oauth_ but create others resources needed for integration like _secrets_, _ConfigMaps_ and _Cron Job_ for syncying groups.

.LDAP folder
image::ldap-folder-1.png[]

NOTE: _Sync Waves_ are needed to make sure those resources like _secrets_ and _ConfigMap_ for authentication exists when we update _Oauth_ configuration, otherwise *_openshift-authentication_* cluster operator will become *_Degraded_*.

When you create the _values-oauth.yaml_ and update existing Application you can notice how they are created in phases and not all at the same time.

For groups _syncying_, _Cron Job_ shows last 5 executions, including the state according to the result: _failed_ or _success_.

Furthermore you can notice some resources with *_No status_* (_white fields_). Those resources are created by OpenShift for configuration issues but they are not managed by Argo, that is why Argo is not tracking them. _Resource tracking_ has been set to _annotation+label_ on _argocd-infra_ instance in the _SNO_.

.LDAP not tracked resources
image::ldap-resources.png[]

NOTE: Learn more about https://argo-cd.readthedocs.io/en/stable/user-guide/resource_tracking/[Resource tracking].

Once every resource is deployed, verify _authentication cluster operator_ is _OK_:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
oc get co 
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
authentication                             4.17.19   True        False         False      2d7h    
baremetal                                  4.17.19   True        False         False      2d10h   
cloud-controller-manager                   4.17.19   True        False         False      2d10h   
cloud-credential                           4.17.19   True        False         False      2d10h   
cluster-autoscaler                         4.17.19   True        False         False      2d10h   
config-operator                            4.17.19   True        False         False      2d10h   
console                                    4.17.19   True        False         False      2d7h    
control-plane-machine-set                  4.17.19   True        False         False      2d10h   
csi-snapshot-controller                    4.17.19   True        False         False      2d10h   
dns                                        4.17.19   True        False         False      2d10h   
etcd                                       4.17.19   True        False         False      2d10h   
image-registry                             4.17.19   True        False         False      2d10h   
ingress                                    4.17.19   True        False         False      111m    
insights                                   4.17.19   True        False         False      2d10h   
kube-apiserver                             4.17.19   True        False         False      2d10h   
kube-controller-manager                    4.17.19   True        False         False      2d10h   
kube-scheduler                             4.17.19   True        False         False      2d10h   
kube-storage-version-migrator              4.17.19   True        False         False      2d10h   
machine-api                                4.17.19   True        False         False      2d10h   
machine-approver                           4.17.19   True        False         False      2d10h   
machine-config                             4.17.19   True        False         False      2d10h   
marketplace                                4.17.19   True        False         False      2d10h   
monitoring                                 4.17.19   True        False         False      2d10h   
network                                    4.17.19   True        False         False      2d10h   
node-tuning                                4.17.19   True        False         False      2d10h   
openshift-apiserver                        4.17.19   True        False         False      2d10h   
openshift-controller-manager               4.17.19   True        False         False      2d10h   
openshift-samples                          4.17.19   True        False         False      2d10h   
operator-lifecycle-manager                 4.17.19   True        False         False      2d10h   
operator-lifecycle-manager-catalog         4.17.19   True        False         False      2d10h   
operator-lifecycle-manager-packageserver   4.17.19   True        False         False      111m    
service-ca                                 4.17.19   True        False         False      2d10h   
storage                                    4.17.19   True        False         False      2d10h
----

Take a look also to the _YAML_ definition:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
oc get oauth cluster -o yaml 
----

It might take a while till authentication pods restart, if you see oauth is not progressing manually restart openshift-authentication pods.

- _Log out_ and _Log in_ back (E.g. https://console-openshift-console.apps.sno-<clusterID>.<domain>). 

- Then you should see new Identity Providers listed in your _SNO_.

- You can try _Log in_ _Keycloack_ server with credentials: 

  myuser-<clusterID>/myuser-<clusterID>

NOTE: _LDAP pods_ can take some minutes to restart.

- You can try _Log in_ to _LDAP_ server (FreeIPA) with credentials:

  paul/Passw0rd 

NOTE: _paul_ is _admin_ user.

- You can try _Log in_ to _LDAP_ server (FreeIPA) with credentials:

  mark/Passw0rd
  
NOTE: _mark_ is _viewer_ user.

[#deployoperators]
== 4.2 Deploy Operators

Once authentication is configured, we are going to deploy some operators. Operators Helm charts use _range values_ so we can define as many operators as we want on _values_ section.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cat ~/helm-infra-gitops-workshop/charts/operators/templates/operators/subscription.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
...
{{- range $key, $val := $.Values.operators }}
...
----

We are going to deploy *Tekton*, *Kiali*, *Jaeger*, *Service Mesh* and *Nmstate* operators. Furthermore we are going to deploy *_Service Mesh Control Plane_* and *_Service Mesh Member Roll_* instances and an example application called _Bookinfo_ for Service Mesh.

Go to your working branch in the https://github.com/alvarolop/helm-infra-gitops-workshop[helm-infra-gitops-workshop] repository and create a file called *values-operators.yaml*:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/helm-infra-gitops-workshop
vi values-operators.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
operators:
  enabled: true
  operators:
    tekton:
      enabled: true
    knative:
      enabled: true
    kiali:
      enabled: true
    jaeger:
      enabled: true
    servicemesh:
      enabled: true 
    nmstate:
      enabled: true  
  istio:
    enabled: true      
---- 

Once the _values-operators.yaml_ exists in your repository, push it and create a new Application in the _argocd-infra_ instance:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/helm-infra-gitops-workshop
git add .
git commit -m "operators"
git push origin sno-<clusterID>
----

Log in the _argocd-infra_ instance (E.g. https://argocd-infra-server-openshift-gitops.apps.sno-<clusterID>.<domain>)

To create _sno-<clusterID>-operators_ Application, once logged in, click on:

- *_New app_* -> *_Edit as Yaml_*

Replace the following in the Application:

- *<clusterID> -> Assigned cluster*
- *<your_user> -> Your GitHub user account*

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: sno-<clusterID>-operators
  namespace: openshift-gitops
spec:
  destination:
    server: 'https://kubernetes.default.svc'
  project: default
  source:
    helm:
      valueFiles:
        - values-operators.yaml
    path: .
    repoURL: 'https://github.com/<your_user>/helm-infra-gitops-workshop.git'
    targetRevision: sno-<clusterID>
  syncPolicy:
    automated:
      prune: true
      selfHeal: true   
---- 

- Then hit *_Save_* -> *_Create_*

Helm charts includes _subcription_ definition for each operator in the last version available in stable channel, while _Install Plan_ is set to _Automatic_ so we do not need to manually approve installation. This is all set in _values.yaml_ file as parameters so we can use these Charts for different installation methods by overriding those values.

In order to deploy the *_Bookinfo_* application successfully, several prerequisites must be met. These include the installation of the operator, as well as the proper configuration of the _Service Mesh Control Plane_ and _Service Mesh Member Roll_. To ensure that these prerequisites are met, *_Sync Waves_* and *_Health Checks_* play a crucial role in the deployment process.

If _Sync Waves_ are not configured properly it will try to create resources whose _API_ still does not exists in the cluster.

Once operators are installed, you can view them as well with the _Install Plan_ managed by Argo:

- Log in _argocd-infra_ instance console: (E.g. https://argocd-infra-server-openshift-gitops.apps.sno-<clusterID>.<domain>)

- Click on _sno-<clusterID>-operators_ Application

.Installed operators view in argocd-infra instance
image::operators-install-plan.png[]

Then, deploy _Bookinfo_ app using _argocd-apps_ instance. You will realize you only need to deploy apps components as namespace is already managed by _argocd-infra_ instance:

- Log in *_argocd-apps_* instance console: (E.g. https://argocd-apps-server-openshift-operators.apps.sno-<clusterID>.<domain>)

Deploy the _sno-<clusterID>-bookinfo_ Application, once logged in, click on:

- *_New app_* -> *_Edit as Yaml_*

Replace the following in the Application:

- *<clusterID> -> Assigned cluster*
- *<your_user> -> Your GitHub user account*

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: sno-<clusterID>-bookinfo
  namespace: openshift-operators
spec:
  destination:
    server: 'https://kubernetes.default.svc'
  project: default
  source:
    helm:
      parameters:
        - name: bookinfo.enabled
          value: 'true'
    path: .
    repoURL: 'https://github.com/<your_user>/helm-infra-gitops-workshop.git'
    targetRevision: sno-<clusterID>
  syncPolicy:
    automated:
      prune: true
      selfHeal: true   
---- 

- Then hit *_Save_* -> *_Create_*

[#namespace]
== 4.3 Namespace Configuration

Part of day 2 configurations are setting _namespace scoped_ configurations for managing _networking_ and _quotas_ for apps, as well as setting _RBAC_.

In this example, based on the last application deployment, we are going to deploy some resources and objects quotas by namespace.

Therefore we are going to set some cluster and local roles.

Finally we are going to deploy a *_Network Policy_* to prevent traffic to the application. You can try _enabling/disabling_ this feature to see how traffic is allowed and denied.

Take a look to the _Network Policy_ Helm chart: 

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/helm-infra-gitops-workshop
cat charts/namespace/templates/app/network-policy.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
{{- if $.Values.global.render -}}
{{- if $.Values.networkpolicy.enabled -}}
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: {{ $.Values.networkpolicy.name }}
  namespace: {{ $.Values.networkpolicy.namespace }}
spec:
  podSelector: {}
  ingress: []
{{- end -}}
{{- end -}}  
----

The previous *_Network Policy_* blocks all incoming traffic by selecting all pods in the namespace and denying all ingress traffic. This is controlled by the *_networkpolicy.enabled_* value in the *_values-namespace.yaml_* file we are going to create bellow.


Go to your working branch in the https://github.com/alvarolop/helm-infra-gitops-workshop[helm-infra-gitops-workshop] repository and create a file called *values-namespace.yaml*:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/helm-infra-gitops-workshop
vi values-namespace.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
namespace:
  enabled: true #Enable dependency
  networkpolicy:
    enabled: true
---- 

Once the _values-namespace.yaml_ exists in your repository, push it and create a new Application in the _argocd-infra_ instance:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/helm-infra-gitops-workshop
git add .
git commit -m "namespace configuration"
git push origin sno-<clusterID>
----

Log in the _argocd-infra_ instance (E.g. https://argocd-infra-server-openshift-gitops.apps.sno-<clusterID>.<domain>)

To create _sno-<clusterID>-namespace_ Application, once logged in, click on:

- *_New app_* -> *_Edit as Yaml_*

Replace the following in the Application:

- *<clusterID> -> Assigned cluster*
- *<your_user> -> Your GitHub user account*

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: sno-<clusterID>-namespace
  namespace: openshift-gitops
spec:
  destination:
    server: 'https://kubernetes.default.svc'
  project: default
  source:
    helm:
      valueFiles:
        - values-namespace.yaml 
    path: .
    repoURL: 'https://github.com/<your_user>/helm-infra-gitops-workshop.git'
    targetRevision: sno-<clusterID>
  syncPolicy:
    automated:
      prune: true
      selfHeal: true                                                       
----

- Then hit *_Save_* -> *_Create_*

Then deploy an _example-app_ on *_argocd-apps_* instance (E.g. https://argocd-apps-server-openshift-operators.apps.sno-<clusterID>.<domain>)

Deploy the _sno-<clusterID>-app_ Application, once logged in, click on:

- *_New app_* -> *_Edit as Yaml_*

Replace the following in the Application:

- *<clusterID> -> Assigned cluster*
- *<your_user> -> Your GitHub user account*

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: sno-<clusterID>-app
  namespace: openshift-operators
spec:
  destination:
    server: 'https://kubernetes.default.svc'
  project: default
  source:
    helm:
      parameters:
        - name: app.enabled
          value: 'true' 
    path: .
    repoURL: 'https://github.com/<your_user>/helm-infra-gitops-workshop.git'
    targetRevision: sno-<clusterID>
  syncPolicy:
    automated:
      prune: true
      selfHeal: true                                                       
---- 

- Then hit *_Save_* -> *_Create_*

Once you update the Application you want be able to create more than _4 pods_ in namespace _app_. Try to update replicas to _5_ in _Deployment_ to see if _quota_ has been correctly applied by Argo.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/helm-infra-gitops-workshop
vi charts/app/values.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
app:
...
  replicas: 5
...
----

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/helm-infra-gitops-workshop
git add .
git commit -m "replicas scaled up"
git push origin sno-<clusterID>
----

Once you update the replicas in the _Deployment_ you should see _4 of 5 pods_:

.Quota
image::quota-applied.png[]

Deployment never progess to _5 replicas_, and Argo stays in _Progressing_ trying to reconcile a not allowed values of replicas. Finally *set it back to _1 replica_*.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/helm-infra-gitops-workshop
vi charts/app/values.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
app:
...
  replicas: 1
...
----

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/helm-infra-gitops-workshop
git add .
git commit -m "replicas scaled down"
git push origin sno-<clusterID>
----

Then if you try to navigate to _app_ route you will see you are not allowed:

IMPORTANT: Replace <clusterID> and <domain> when needed.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
oc get route back-springboot -n app
----

.app not responsible
image::traffic-not-allowed.png[]

Then disable _Network Policy_ and verify how you have traffic access:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/helm-infra-gitops-workshop
vi values-namespace.yaml
----

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
...
namespace:
  enabled: true #Enable dependency
  networkpolicy:
    enabled: false
...
----

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/helm-infra-gitops-workshop
git add .
git commit -m "network policy disabled"
git push origin sno-<clusterID>

oc get route back-springboot -n app
----

.app available
image::traffic-allowed.png[]

[#monitoring]
== 4.4 Configure Monitoring

Now we are going to deploy some basic configuration about monitoring.

In OpenShift 4, monitoring is enabled by default. However there are lots of configurations we can modify and configure non default _user-defined_ projects monitoring stack.

NOTE: Take a look to the https://docs.openshift.com/container-platform/4.17/observability/monitoring/about-ocp-monitoring/monitoring-stack-architecture.html[Monitoring documentation].

In the first place, we are going to enable _user-defined_ projects monitoring. Then we will create an *_example-app_*, with a _Service Monitor_ and a custom _Prometheus Rule_ in order to gather metrics from the _example-app_ application and trigger an alarm based on an specific metric value.

Go to your working branch in the https://github.com/alvarolop/helm-infra-gitops-workshop[helm-infra-gitops-workshop] repository and create a file called *values-monitoring.yaml*:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/helm-infra-gitops-workshop
vi values-monitoring.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
monitoring:
  enabled: true #Enable dependency   
---- 

Once the _values-monitoring.yaml_ exists in your repository, push it and create a new Application in the _argocd-infra_ instance:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/helm-infra-gitops-workshop
git add .
git commit -m "monitoring"
git push origin sno-<clusterID>
----

Log in the _argocd-infra_ instance (E.g. https://argocd-infra-server-openshift-gitops.apps.sno-<clusterID>.<domain>)

To create _sno-<clusterID>-monitoring_ Application, once logged in, click on:

- *_New app_* -> *_Edit as Yaml_*

Replace the following in the Application:

- *<clusterID> -> Assigned cluster*
- *<your_user> -> Your GitHub user account*

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: sno-<clusterID>-monitoring
  namespace: openshift-gitops
spec:
  destination:
    server: 'https://kubernetes.default.svc'
  project: default
  source:
    helm:
      valueFiles:
        - values-monitoring.yaml
    path: .
    repoURL: 'https://github.com/<your_user>/helm-infra-gitops-workshop.git'
    targetRevision: sno-<clusterID>
  syncPolicy:
    automated:
      prune: true
      selfHeal: true                                                            
---- 

- Then hit *_Save_* -> *_Create_*

Configuring the monitoring application is quite straightforward since it does not have any direct dependencies on other objects. As such, you do not need to worry about setting up _Sync Waves_, which are typically used to manage the order in which objects are deployed to avoid issues with dependencies.

Then navigate to OpenShift _SNO_ console (E.g. https://console-openshift-console.apps.sno-<clusterID>.<domain>) to verify those objects deployed in the _argocd-infra_ instance exist on the _SNO_ and if _Service Monitor_ is scraping your metrics properly:

Once the _Service Monitor_ is created, the respective metrics should be found in the _SNO_ OpenShift Console (*_Observe_* -> *_Metrics_*). For example, it is possible to find the *_tomcat_sessions_active_current_sessions metric_*:

.Metrics
image::service-monitor-1.png[]

The respective alert to the _Prometheus rule_ created should be found in the OpenShift Console (*_Observe_* -> *_Alerting_*). For example, it is possible to find the *App1SessionsAlert* alert:

.Prometheus rule
image::promethus-alert-1.png[]

In this case, it is possible to see that this alarm is firing because the metric *tomcat_sessions_alive_max_seconds* is equal to *0*.

NOTE: Please pay special attention to alerting best practices included in the following https://docs.openshift.com/container-platform/4.17/observability/monitoring/about-ocp-monitoring/key-concepts.html#tips-for-optimizing-alerting-rules-for-core-platform-monitoring_key-concepts[link].

[#vault]
== 4.5 Vault Configuration

IMPORTANT: As prerequisite make sure you have deployed the *Vault application* listed at the end of *_3.2 Helm Charts subsection_*.

_Vault_ by _Hashicorp_ is a tool that allows to store and encrypt secrets to secure applications and protect sensitive data. Vault server stores the sensitive data while a special plugin for Argo retrieves this information when creating objects thanks to the use of paths and references so we do not leave sensitive information visible in the code repository. 

First of all there is a running instance of Vault on _Argo Hub_ cluster. This server stores sensitive data for configuring _secrets_ and _ConfigMaps_, while on your _SNO_ you can see a secret containing credentials for authenticating with Vault, a _ConfigMap_ with plugin for using Helm with Vault and Argo, and a special configuration on ArgoCD instance.

Those resources are required to implement _ArgoCD Vault plugin_. This plugin allows using _placeholders_ with path to secrets on _YAML_ fields where the secret should be replaced, and the plugin is in charge of this substitution.

There are several ways of installing it, as *_sidecars plugin_* or as _ConfigMap plugin_.

NOTE: https://argo-cd.readthedocs.io/en/stable/operator-manual/config-management-plugins/#installing-a-config-management-plugin[_ConfigMap plugin_] will be deplecated in the future.

So this installation approach follows the method *_initContainer_ + _sidecar_*.

NOTE: https://argocd-vault-plugin.readthedocs.io/en/stable/installation/#initcontainer-and-configuration-via-sidecar[initContainer + sidecar] documentation.

ConfigMap *_cmp-plugin_* defines the plugin that will be mounted in the sidecar container:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
oc login -u admin -p <pass> https://api.sno-<clusterID>.<domain>:6443
oc get -n openshift-operators configmap cmp-plugin -o yaml 
----  

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cmp-plugin #To be defined parameters
  namespace: openshift-operators
data:
  plugin.yaml: |
    apiVersion: argoproj.io/v1alpha1
    kind: ConfigManagementPlugin
    metadata:
      name: argocd-vault-plugin-helm
    spec:
      allowConcurrency: true
      discover:
        find:
          command:
            - sh
            - "-c"
            - "find . -name 'Chart.yaml' && find . -name 'values.yaml'"
      init:
       command:
          - bash
          - "-c"
          - |
            helm repo add bitnami https://charts.bitnami.com/bitnami
            helm dependency build
      generate:
        command:
          - bash
          - "-c"
          - |
            helm template . $ARGOCD_ENV_HELM_VALUES | # values passed in Application
            argocd-vault-plugin generate -s openshift-operators:argocd-vault-plugin-credentials - # generate using plugin + credentials
      lockRepo: false
----      

Secret *_argocd-vault-plugin-credentials_* defines Vault Server address, authentication type (_approle_) and role credentials:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
oc get -n openshift-operators secret argocd-vault-plugin-credentials -o yaml 
----  

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
kind: Secret
apiVersion: v1
metadata:
  name: argocd-vault-plugin-credentials #To be defined parameters
  namespace: openshift-operators #argocd namespace
type: Opaque
stringData:
  VAULT_ADDR: "http://vault-vault.apps.argo-hub.sandbox1444.opentlc.com"
  AVP_TYPE: vault
  AVP_AUTH_TYPE: approle
  AVP_ROLE_ID: <your_role_id>
  AVP_SECRET_ID: <your_secret_id>
----  

NOTE: Here you can take a look to several https://developer.hashicorp.com/vault/docs/concepts/auth[Authentication Methods].

Then it is necessary to configure using this plugin on ArgoCD:

IMPORTANT: In this case, this configuration is already running on your cluster. 

Take a look to the configuration applied by the Application _sno-<clusterID>-bootstrap on your sno  _argocd-infra_ instance where those resources have been already created as part of _bootstrapping_. Do not make any change.

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
...
  repo:
    resources:
      limits:
        cpu: 1000m
        memory: 1024Mi
      requests:
        cpu: 250m
        memory: 256Mi
    env:      
        - name: AVP_AUTH_TYPE # Field from argocd-vault-plugin-credentials secret
          valueFrom:
            secretKeyRef:
              key: AVP_AUTH_TYPE
              name: argocd-vault-plugin-credentials
        - name: AVP_TYPE
          valueFrom:
            secretKeyRef:
              key: AVP_TYPE
              name: argocd-vault-plugin-credentials
        - name: VAULT_ADDR
          valueFrom:
            secretKeyRef:
              key: VAULT_ADDR
              name: argocd-vault-plugin-credentials
        - name: AVP_ROLE_ID
          valueFrom:
            secretKeyRef:
              key: AVP_ROLE_ID
              name: argocd-vault-plugin-credentials        
        - name: AVP_SECRET_ID
          valueFrom:
            secretKeyRef:
              key: AVP_SECRET_ID
              name: argocd-vault-plugin-credentials                  
    mountsatoken: true
    serviceaccount: argocd-repo-server # sa to be used
    sidecarContainers: # sidecar container running plugin 
      - command:
          - /var/run/argocd/argocd-cmp-server
        image: 'quay.io/argoproj/argocd:v2.13.5'
        name: avp-helm              
        volumeMounts:
          - mountPath: /var/run/argocd
            name: var-files
          - mountPath: /home/argocd/cmp-server/plugins
            name: plugins
          - mountPath: /tmp
            name: tmp-dir
          - mountPath: /home/argocd/cmp-server/config
            name: cmp-plugin
          - mountPath: /usr/local/bin/argocd-vault-plugin
            name: custom-tools
            subPath: argocd-vault-plugin
    volumeMounts:
      - mountPath: /usr/local/bin/argocd-vault-plugin
        name: custom-tools
        subPath: argocd-vault-plugin
    volumes:
      - configMap:
          name: cmp-plugin
        name: cmp-plugin
      - emptyDir: {}
        name: custom-tools
      - emptyDir: {}
        name: tmp-dir                  
    initContainers: # init container
      - args:
          - >-
            wget -O /custom-tools/argocd-vault-plugin
            https://github.com/argoproj-labs/argocd-vault-plugin/releases/download/v${AVP_VERSION}/argocd-vault-plugin_${AVP_VERSION}_linux_amd64
            && chmod +x /custom-tools/argocd-vault-plugin && ls -la
            /custom-tools/
        command:
          - sh
          - '-c'
        env:
          - name: AVP_VERSION
            value: 1.18.1
        image: 'alpine:3.8'
        name: download-tools
        volumeMounts:
          - mountPath: /custom-tools
            name: custom-tools               


  configManagementPlugins: | # register plugin
    - name: argocd-vault-plugin
      generate:
        command: ["argocd-vault-plugin"]
        args: ["generate", "./"] 
...
----

So the next step is testing this actually works!

In the https://github.com/alvarolop/helm-infra-gitops-workshop[helm-infra-gitops-workshop] repository, you can find a _secret_ using a Vault placeholder in _charts/vault/values.yaml_:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/helm-infra-gitops-workshop
cat charts/vault/values.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
...
  placeholder: "<password | base64encode>"
  path: "kv-v2/data/demo"
...
----

If you take a look to the existing secret in _vault-secrets_ namespace, as we are telling Application not to use Vault plugin, it is *NOT* replacing the sensitive information:

- Log in OpenShift _SNO_ console (E.g. https://console-openshift-console.apps.sno-<clusterID>.<domain>)

- Hit *_Workshop Users_*. 

- Insert _admin_ and _<pass>_ provided by instructor and click *_Log in_* button.

- Change to project *_vault-secrets_* -> In _Search_ bar type: *_secret_* -> Then click on the _secret_ called *_vault_*.

.Vault plugin secret
image::secret-vault.png[]

So we need to modify existing Application _sno-<clusterID>-vault_ in _argocd-apps_ instance (E.g. https://argocd-apps-server-openshift-operators.apps.sno-<clusterID>.<domain>) to use *plugin*. 

NOTE: This modification applies to *Vault application* deployed at the end of *_3.2 Helm Charts subsection_*.

- Click *_sno-<clusterID>-vault_* -> *_App details_* -> *_Manifest_* -> then *_Edit_*, and add:

IMPORTANT: Replace only the plugin section. And change _<your_user>_ for your GitHub user account.

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
...
  source:
    repoURL: 'https://github.com/<your_user>/helm-infra-gitops-workshop.git'
    path: .
    targetRevision: sno-<clusterID>
    plugin:
      env:
        - name: HELM_VALUES
          value: >-
            --set vault.enabled=true 
...            
----

As you can see, this application is slightly different to the last one used. This is due to we need to pass _values_ files and parameters so *_argocd-vault-plugin-helm_* secret can used them
to render Helm charts. This might looks slightly different depending on you repository structure. 

NOTE: If you do not need to pass any values you can simply invoke *"plugin: {}"*.

After applying this new application, it will be _Out of Sync_ for some seconds. Once it is _Synced_, navigate to your OpenShift _SNO_ and verify Vault has replaced secret data properly.
You can try to delete it and see how it is created. 

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----

oc get route vault -n vault-secrets 
curl vault-vault-secrets.apps.sno-<clusterID>.<domain>
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
The password value is: cGFzc3dvcmQxMjM=
----

It might take a while, if you keep on seeing plain text, navigate to the SNO console and check it there.

image::data-secret.png[]

Finally you can ask your instructor to update this secret on Vault server, try a *_Hard refresh_* on _argocd-apps_ instance and see how it is updated.

[#appset]
== 4.6 Day 2 with ApplicationSet

Until now, you have applied day 2 operations by creating single Applications by hand. However there is an easier way to render those apps using ApplicationSets.

Checkout to *main-day2* branch in this https://github.com/alvarolop/workshop-gitops-content-deploy[workshop-gitops-content-deploy] repository to take a look:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
git checkout main-day2   
----  

Navigate to the ApplicationSet folder and take a look to the newly added day2-sno-as file.

Replace the following in the ApplicationSet and save it:

- *<clusterID> -> Assigned cluster*
- *<your_user> -> Your GitHub user account*

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
vi cluster-addons/cluster-addons-as/day2-sno-as.yaml
----  

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
---
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: day2-sno-<clusterID>
  namespace: openshift-operators
spec:
  generators:
  - git:
      repoURL: https://github.com/<your_user>/workshop-gitops-content-deploy.git
      revision: sno-<clusterID>-setup
      files:
      - path: "cluster-definition/**/cluster.json"
  template:
    metadata:
      name: 'day2-{{cluster.name}}-a'
    spec:
      project: '{{project}}'
      source:
        repoURL: https://github.com/<your_user>/workshop-gitops-content-deploy.git
        targetRevision: sno-<clusterID>-setup
        path: cluster-addons/day2-as
      destination:
        server: '{{cluster.address}}'
      syncPolicy:
        automated:
          prune: true
          selfHeal: true   
----  

This ApplicationSet render _'N'_ configurations for _'N'_ managed clusters:

.Day-2 ApplicationSet
image::diagram-6.png[]

This ApplicationSet applies day 2 configurations by creating Applications for *_Oauth_*, *_Monitoring_* and *_Operators_* on _argocd-infra_ instance on _SNO_.

.Deploy Application as part of Day-2 ApplicationSet
image::diagram-7.png[]

If you navigate to the Charts folder, *_~/workshop-gitops-content-deploy/cluster-addons/day2-as/_*, you will see you are not creating objects itself but Applications. LetÂ´s test it.

Save and commit your last changes:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
git add .
git commit -m "wip"
----  

Go back to your working branch *_sno-<clusterID>-setup_* and merge it with *main-day2* branch. 

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
git checkout sno-<clusterID>-setup
git merge main-day2
----  

IMPORTANT: As part of this *merge* action you will find file conflicts that has to be solved by hand. Take a look at them carefully and solve them one by one, make sure your workshop data like *_<your_user>_*, *_<clusterID>_*, *_<domain>_*, folder *cluster-definition/sno-<clusterID>* are properly replaced. 

NOTE: Tools as _Visual Code Studio_ can make this step easier, if you are not already using it at this point of the workshop.

Now, you must see this extra ApplicationSet in *_~/workshop-gitops-content-deploy/cluster-addons/cluster-addons-as/_*, plus a new *_~/workshop-gitops-content-deploy/cluster-addons/day2-as/_* folder on Charts.

.Day-2 extra folder and ApplicationSet
image::day2-folder-structure-1.png[]

If you take a look to this ApplicationSet which will be created in _argocd-infra_ instance on destination cluster _SNO_, you will see that *_generators_* label iterates over _config-definition_ folder on root directory and uses every child folder name (day 2 operators) to name the Application template and it takes the values file from the *_cluster-definition/sno-<clusterID>/config.json_* file:

Remember to replace with your cluster configuration data as required.

- *<clusterID> -> Assigned cluster*
- *<your_user> -> Your GitHub user account*

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
vi cluster-addons/day2-as/application-set-day2.yaml
----  

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: day2-as-sno-<clusterID>
  namespace: openshift-gitops
spec:
  generators:
  - git:
      repoURL: https://github.com/<your_user>/workshop-gitops-content-deploy.git
      revision: sno-<clusterID>-setup
      files:
      - path: "config-definition/**/config.json" 
  template:
    metadata:
      name: 'sno-<clusterID>-{{path.basename}}'
    spec:
      project: default
      source:
        repoURL: https://github.com/<your_user>/helm-infra-gitops-workshop.git
        targetRevision: sno-<clusterID>
        path: .
        helm:
          valueFiles:
            - '{{valuesFile}}'        
      destination:
        server: 'https://kubernetes.default.svc'
        namespace: openshift-gitops
      syncPolicy:
        automated:
          prune: true
          selfHeal: true 
----  

Then push to your changes to working branch *_sno-<clusterID>-setup_*.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
git add .
git commit -m "day 2 with ApplicationSet"
git push origin sno-<clusterID>-setup
----  

Finally, navigate to _Argo Hub_ *_argocd_* instance and see the recently created ApplicationSet, then navigate to *_argocd-infra_* instance on _SNO_ and see the Applications managed by the Application generated by ApplicationSet.

It might take a while till it syncs all the resources affected by the changes. Once everything is synced, you can try deleting _sno-<clusterID>-monitoring_ Application on argocd-infra instance to verify ApplicationSet recreates it.

.Complete view of GitOps approach for infraestructure and application deployment
image::diagram-8.png[]

[#summary]
== 4.7 Summary

Throughout this workshop, we have covered the basics of deploying _Day 2 operations_ and _applications_ using ArgoCD, with a focus on GitOps infrastructure configuration. We have explored topics such as configuring an _Argo Hub_, setting up managed clusters, and automating deployment using ApplicationSet and Application resources.

We hope that this workshop has provided you with a introduction to these concepts and given you the knowledge and tools to explore them further on your own.