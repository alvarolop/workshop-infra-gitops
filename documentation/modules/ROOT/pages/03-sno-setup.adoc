= 3. Deploy Managed Cluster Setup

[#managedconfiguration]
== 3.1 Managed Cluster Configuration

_Argo Hub_ Application has been already synced, so you just need to be focus on managing your Single Node OpenShift.

Let's make some modifications in the code to start deploying _GitOps_ in the managed cluster. Open the _workshop-gitops-content-deploy_ folder and make sure all the changes are done in the correct branch _sno-<name>-setup_:

NOTE: What is https://www.redhat.com/en/topics/devops/what-is-gitops[GitOps]?

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
git checkout sno-<name>-setup
----

As every user is deploying its own bootstrap file, they must have an unique name and they should be deployed on your scoped project where your user has privileges.

IMPORTANT: Change *cluster-definition/sno-<name>* folder as your assigned cluster as this value will be used as a parameter for ApplicationSet.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
E.g.: mv cluster-definition/sno-<name> cluster-definition/sno-1
----

IMPORTANT: Always replace _<name>_ and _<domain>_ by your assigned cluster. 

Update from _cluster-definition/sno-<name>/cluster.json_:

- *<name> -> Assigned cluster*
- *<domain> -> Assigned domain cluster*

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
vi cluster-definition/sno-<name>/cluster.json
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
{
  "environment": "dev",
  "cloud": "aws",
  "project": "project-sno-<name>",
  "region": "eu-central-1",
  "cluster": {
    "name": "sno-<name>",
    "address": "https://api.sno-<name>.<domain>:6443"
  }
}
----

You must also update _global-config/bootstrap-a/managed-setup.yaml_. Note how this application deploys to a different namespace and argo instance compared to _hub-setup_ application.

Update from _global-config/bootstrap-a/managed-setup.yaml_:

- *<name> -> Assigned cluster*
- *<your_user> -> Your GitHub user account*

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
vi global-config/bootstrap-a/managed-setup.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: managed-setup-a-<name>
  namespace: openshift-operators
spec:
  destination:
    namespace: openshift-operators
    server: https://kubernetes.default.svc
  project: project-sno-<name>
  source:
    repoURL: https://github.com/<your_user>/workshop-gitops-content-deploy.git
    targetRevision: sno-<name>-setup
    path: cluster-addons/cluster-addons-as/
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
----      

The source path of this Application points to the ApplicationSet used.

ApplicationSet is a Custom Resource that can be used to generate Applications to multiple clusters from multiple origins. ApplicationSet controller is installed alongside ArgoCD even though it must be explicitly enabled. 

ApplicationSet is a helpful approach for multi cluster management, and it supplements ArgoCD by adding additional features in support of _cluster-administrator-focused_ scenarios. Some of those are:

- Ability to use a single Kubernetes manifest to target multiple Kubernetes clusters with ArgoCD.

- Ability to use a single Kubernetes manifest to deploy multiple applications from one or multiple Git repositories with ArgoCD.

- Improved support for monorepos (multiple ArgoCD Application resources defined within a single Git repository).

- Improves ability of individual cluster tenants to deploy applications using ArgoCD (without needing to involve privileged cluster admin in enabling the destination servers).

ApplicationSet uses generators to create Application with multiple origins and destinations using templating. 

Generator used may vary depending on your use case, in this example
we are using _Git Files_ generator so cluster definition values are used as parameters. In case we had _'N' cluster-definition_ files, the ApplicationSet would generate _'N'_ Applications automatically.

NOTE: Find more about https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/Generators/[Generators]

Now we are going to take a look at _cluster-addons/cluster-addons-as/_ in this folder we can expect two AplicationSet:

.Folder structure
image::workshop-content-deploy-folders-1.png[]

The first ApplicationSet _cluster-addons/cluster-addons-as/gitops-setup-sno-as.yaml_ deploys the _openshift-gitops_ operator and an initial non default _cluster wide_ instance for infrastructure operations.

Update from _cluster-addons/cluster-addons-as/gitops-setup-sno-as.yaml_:

- *<name> -> Assigned cluster*
- *<your_user> -> Your GitHub user account*

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
vi cluster-addons/cluster-addons-as/gitops-setup-sno-as.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: gitops-setup-sno-<name>
  namespace: openshift-operators
spec:
  generators:
  - git:
      repoURL: https://github.com/<your_user>/workshop-gitops-content-deploy.git
      revision: sno-<name>-setup
      files:
      - path: "cluster-definition/**/cluster.json"
  template:
    metadata:
      name: 'gitops-setup-{{cluster.name}}-a'
    spec:
      project: '{{project}}'
      source:
        repoURL: https://github.com/<your_user>/workshop-gitops-content-deploy.git
        targetRevision: sno-<name>-setup
        path: cluster-addons/charts/gitops-setup 
      destination:
        server: '{{cluster.address}}'
      syncPolicy:
        automated:
          prune: true
          selfHeal: true       
----    

The second ApplicationSet _cluster-addons/cluster-addons-as/bootstrap-sno-as.yaml_ deploys an Application in the _argocd-infra_ instance on manged cluster, called _sno-setup_, with configuration like: _RBAC_, a second _argocd-apps_ instance, namespaces and vault.

.sno-setup Application
image::diagram-4.png[]

Update from _cluster-addons/cluster-addons-as/bootstrap-sno-as.yaml_:

- *<name> -> Assigned cluster*
- *<your_user> -> Your GitHub user account*

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
vi cluster-addons/cluster-addons-as/bootstrap-sno-as.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: bootstrap-sno-<name>
  namespace: openshift-operators
spec:
  generators:
  - git:
      repoURL: https://github.com/<your_user>/workshop-gitops-content-deploy.git
      revision: sno-<name>-setup
      files:
      - path: "cluster-definition/**/cluster.json"
  template:
    metadata:
      name: 'bootstrap-{{cluster.name}}-a'
    spec:
      project: '{{project}}'
      source:
        repoURL: https://github.com/<your_user>/workshop-gitops-content-deploy.git
        targetRevision: sno-<name>-setup
        path: cluster-addons/charts/bootstrap-app
      destination:
        server: '{{cluster.address}}'
      syncPolicy:
        automated:
          prune: true
          selfHeal: true       
----     

Then update _cluster-addons/charts/bootstrap-app/values.yaml_ file with your assigned data too:

- *<name> -> Assigned cluster*
- *<your_user> -> Your GitHub user account*

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
vi cluster-addons/charts/bootstrap-app/values.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
clusters:
  sno-<name>:
    applicationNamespace: openshift-gitops
    namespace: ''
    destination: 'https://kubernetes.default.svc'
    project: default
    code:
      repo: https://github.com/<your_user>/workshop-gitops-content-deploy.git
      path: cluster-addons/charts/bootstrap
      target: sno-<name>-setup
----

And finally replace values in bootstrap _cluster-addons/charts/bootstrap/values.yaml_:

- *<domain> -> Assigned domain cluster*

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
vi cluster-addons/charts/bootstrap/values.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
...
vault: 
  vault_addr: "http://vault-vault.apps.argo-hub.<domain>"
  avp_type: vault
...
----

This ApplicationSet deploys an Application on the recently deployed instance on managed cluster to deploy and manage a second instance for applications.

Then navigate under source path to take a look to the Helm charts used for deploying GitOps and setting up the initial configuration for managed clusters.

.GitOps Helm Charts
image::workshop-content-deploy-folders-2.png[]

NOTE: ApplicationSet controller is not enabled by default and must be configured on ArgoCD instance.

[#helmcharts]
== 3.2 Helm Charts

A *Helm chart* is a set of _YAML_ manifests and templates that describes Kubernetes resources (Deployments, Secrets, _CRDs_, etc.) and defined configurations needed for the Kubernetes application.

In the _argocd_ instance of _Argo Hub_, the first Helm chart is *_gitops-setup_*, which deploys _openshift-gitops_ operator on managed clusters. This chart is intented to deploy any kind of operator, even though in this case we are only deploying _openshift-gitops_ operator.

If you navigate to _cluster-addons/charts/gitops-setup/templates/operators/subscription.yaml_ resource you will see there is a global value for applying _env_ variables for _GitOps_. 

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
cat cluster-addons/charts/gitops-setup/templates/operators/subscription.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
{{- range $key, $val := $.Values.operators }}
{{- if $val.enabled }}
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: {{ $key }}
  namespace: {{ $val.namespace }}
  {{- if $.Values.argocd.enabled }}
  annotations:
    argocd.argoproj.io/sync-wave: "-4"
  {{- end }}
spec:
  channel: {{ $val.channel }}
  installPlanApproval: {{ $val.approval }}
  name: {{ $val.name }}
  source: redhat-operators
  sourceNamespace: openshift-marketplace
{{- if $.Values.operators.gitops.enabled }}
  config:
    env:
    - name: ARGOCD_CLUSTER_CONFIG_NAMESPACES
      value: openshift-gitops
    - name: DISABLE_DEFAULT_ARGOCD_INSTANCE
      value: "true"      
{{- end }}
{{- end }}
{{- end }}
----

These configuration values disable the default ArgoCD instance and enables a new instance to be _cluster wide_. This means this _Argo Application Controller ServiceAccount_ will have permissions to work in all namespaces within the cluster.

By default any new instance created is namespace scoped, this means you will only be allowed to deploy within your namespace. If you want to deploy across all namespace
you need to change this configuration to make the instance _cluster wide_. Additionally your _Argo ServiceAccount_ may not have privileges enough to work with cluster wide resources and you might need to assign a new _Role Binding_ for it.

You can either create a custom _Role Binding_ or labelling any managed namespace by Argo so it will create this _Role Binding_ automatically only for that namespace.

After setting this global variable you can see a new _Cluster Role Binding_ for this _ServiceAccount_ and this configuration on _Argo Hub_ console in _openshift-operators_ namespace.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
oc get clusterrolebinding openshift-gitops-openshift-gitops-argocd-application-controller -n openshift-operators -o yaml
----

It is possible also to take a look in the _Argo Hub_ web console (https://console-openshift-console.apps.argo-hub.<domain>/):

.Cluster Role Binding
image::cluster-wide-role-binding.png[]

NOTE: Take a look to:
https://developers.redhat.com/articles/2023/03/06/5-global-environment-variables-provided-openshift-gitops#5_environment_variables__overview[Global Env Vars], https://docs.openshift.com/gitops/1.15/argocd_instance/setting-up-argocd-instance.html#gitops-deploy-resources-different-namespaces_setting-up-argocd-instance[How to label namespaces] 
and https://docs.openshift.com/gitops/1.15/declarative_clusterconfig/configuring-an-openshift-cluster-by-deploying-an-application-with-cluster-configurations.html#gitops-additional-permissions-for-cluster-config_configuring-an-openshift-cluster-by-deploying-an-application-with-cluster-configurations[How to create a _Role Binding_].

Once the _openshift-gitops_ operator is running, we need to deploy the ArgoCD instance. To make sure instance is deployed after the operator is running we use _Sync Waves_ and _Custom Resources Healthcheck_.

*_Sync Waves_* are defined on each resource as annotations, and they tell Argo the order in which resources should be applied once the previous resource is already in healthy status.

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
...
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "-5"
...
----

NOTE: You can take a look in detail to the https://argo-cd.readthedocs.io/en/stable/user-guide/sync-waves/[Sync Waves] documentation.

For some specific resources they need a *_Custom Healthcheck_*. Most of the objects only require existing to work but others like _subscriptions_ may exists but not progress to a successful status so we need a _Custom Healthcheck_ to make sure the next _Sync Wave_ does not start till the operators are properly installed.

NOTE: You can take a look in detail to the https://argo-cd.readthedocs.io/en/stable/operator-manual/health/[Custom Healthcheck] documentation.

NOTE: A *_Resource Healthcheck_* is defined in the _argocd_ instace of _Argo Hub_, which is also deployed using Helm charts in _hub-setup/charts/gitops-setup/templates/argocd.yaml_.

Next chart to take a look is _cluster-addons/charts/bootstrap-app_. This chart deploys an Application on the managed cluster _argocd-infra_ instance to apply _bootstrap_ chart.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
cat cluster-addons/charts/bootstrap-app/templates/application.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
{{- range $key, $val := $.Values.clusters }}
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: {{ $key }}-bootstrap
  namespace: {{ $val.applicationNamespace }}
spec:
  destination:
    server: {{ $val.destination }}
    namespace: ''
  project: {{ $val.project }}
  source:
    helm:
      valueFiles:
        - values.yaml
    path: {{ $val.code.path }}
    repoURL: {{ $val.code.repo }}
    targetRevision: {{ $val.code.target }}
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
{{- end }}      
----

Then on _cluster-addons/charts/bootstrap_ folder you can find resources for deploying the second _argocd-apps_ instance in the managed cluster, _namespaces_, _vault_ and _RBAC_ configuration. 

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy/cluster-addons/charts/bootstrap/templates/
----

.Bootstrap resources
image::workshop-content-deploy-folders-3.png[]

The _argocd-apps_ instance definition in _cluster-addons/charts/bootstrap/templates/argocd/argocd.yaml_ is slightly similar to _argocd-infra_ but it has some special customization, let´s take a look:

NOTE: Take a look to the *_resourceCustomizations_* section to review the _Custom Healthcheck_.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
cat cluster-addons/charts/bootstrap/templates/argocd/argocd.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
...
    dex:  
      openShiftOAuth: true # 1
      resources:
        limits:
          cpu: 500m
          memory: 256Mi
        requests:
          cpu: 250m 
          memory: 128Mi
    provider: dex
  resourceTrackingMethod: annotation+label # 2
  applicationSet: # 3
...
  rbac: # 4
    defaultPolicy: ''
    policy: |-
      g, {{ $.Values.argocd.group }}, role:admin
      p, role:operator, applications, get, */*, allow
      p, role:operator, applications, sync, */*, allow
      g, argo-admins, role:admin 
      g, argo-readers, role:readonly
      g, argo-operators, role:operator
      g, argo-dev-operators, role:operator-dev      
    scopes: '[groups]'
...
    sidecarContainers: # 5
      - command:
          - /var/run/argocd/argocd-cmp-server
        image: 'quay.io/argoproj/argocd:v2.4.8'
        name: avp-helm
        volumeMounts:
          - mountPath: /var/run/argocd
            name: var-files
          - mountPath: /home/argocd/cmp-server/plugins
            name: plugins
          - mountPath: /tmp
            name: tmp-dir
          - mountPath: /home/argocd/cmp-server/config
            name: cmp-plugin
          - mountPath: /usr/local/bin/argocd-vault-plugin
            name: custom-tools
            subPath: argocd-vault-plugin
    volumeMounts:
      - mountPath: /usr/local/bin/argocd-vault-plugin
        name: custom-tools
        subPath: argocd-vault-plugin
    volumes:
      - configMap:
          name: cmp-plugin
        name: cmp-plugin
      - emptyDir: {}
        name: custom-tools
      - emptyDir: {}
        name: tmp-dir                  
...              
  configManagementPlugins: | # 6
    - name: argocd-vault-plugin
      generate:
        command: ["argocd-vault-plugin"]
        args: ["generate", "./"]      
  resourceCustomizations: | # 7
    operators.coreos.com/Subscription:
      health.lua: |      
        health_status = {}
        if obj.status ~= nil then
          if obj.status.conditions ~= nil then
            numDegraded = 0
            numPending = 0
            msg = ""
            for i, condition in pairs(obj.status.conditions) do
              msg = msg .. i .. ": " .. condition.type .. " | " .. condition.status .. "\n"
              if condition.type == "InstallPlanPending" and condition.status == "True" then
                numPending = numPending + 1
              elseif (condition.type == "InstallPlanMissing" and condition.reason ~= "ReferencedInstallPlanNotFound") then
                numDegraded = numDegraded + 1
              elseif (condition.type == "CatalogSourcesUnhealthy" or condition.type == "InstallPlanFailed" or condition.type == "ResolutionFailed") and condition.status == "True" then
                numDegraded = numDegraded + 1
              end
            end
            if numDegraded == 0 and numPending == 0 then
              health_status.status = "Healthy"
              health_status.message = msg
              return health_status
            elseif numPending > 0 and numDegraded == 0 then
              health_status.status = "Progressing"
              health_status.message = "An install plan for a subscription is pending installation"
              return health_status
            else
              health_status.status = "Degraded"
              health_status.message = msg
              return health_status
            end
          end
        end
        health_status.status = "Progressing"
        health_status.message = "An install plan for a subscription is pending installation"
        return health_status   
----

Also see:

- 1) Dex uses groups and users defined within Openshift by checking the Oauth server.

- 2) Overrides default tracking method by label to annotation+label.

- 3) Enable ApplicationSet controller.

- 4) Configure argo RBAC.

- 5) Configure vault plugin as a sidecar container.

- 6) Configure new plugin for vault.

- 7) Configure resource healthcheck for Subscription.

As you may notice, this instance contains some parametes for configuring _Vault Plugin_, which we will discuss later, and _RBAC_ model.

*_RBAC_* is defined on _cluster-addons/charts/bootstrap/templates/rbac/_ folder and includes the basic configuration for Argo _RBAC_ and projects.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy/cluster-addons/charts/bootstrap/templates/rbac/
----

.RBAC folder
image::workshop-content-deploy-folders-4.png[]

The *_RBAC_* feature enables restriction of access to ArgoCD resources. ArgoCD does not have its own user management system and has only one _built-in_ user called _admin_. 
The _admin_ user is a superuser and it has unrestricted access to the system. _RBAC_ requires _SSO_ configuration, or one or more local users setup. Once _SSO_ or local users are configured, additional _RBAC_ roles can be defined, and _SSO_ groups or local users can then be mapped to roles.

NOTE: Find more in https://argo-cd.readthedocs.io/en/stable/operator-manual/rbac/[RBAC] documentation.

ArgoCD has two pre-defined roles but _RBAC_ configuration allows defining roles and groups. See below:

- 1) role:readonly - read-only access to all resources

- 2) role:admin - unrestricted access to all resources

Additionally to the defined roles, it is possible to create some specific roles to allow _argo-operators_ and _argo-dev-operators_ group members manage applications in ArgoCD. See the groups in _cluster-addons/charts/bootstrap/values.yaml_ values file:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
vi cluster-addons/charts/bootstrap/values.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
groups:
 argo-admins:
   user: user01 #  Admin permissions in ALL projects and applications
 argo-readers:
   user: user02 # Read-only permissions in ALL projects and applications
 argo-operators:
   user: user03 # View and Sync permission in ALL projects and applications
 argo-dev-operators:
   user: user04 #  View and Sync permission in DEV project and its applications
 argo-integration:
   user: apimanager01 # User has no permissions to see anything in Argo CD but has permissions to create objects in the Openshift Clusters
 cluster-admins:
   user: admin # full admin
----

Then if you navigate to _RBAC_ folder you can see a _Group_ and a _Role Binding_ resource to give _cluster-admin_ permissions on Argo to the admin user configured via _Htpasswd_.

.RBAC folder
image::workshop-content-deploy-folders-4.png[]

NOTE: For _RBAC_ we need to differentiate between global configuration on _argocd-apps_ intance and projects _RBAC_.

If you navigate to rbac section on _argo-apps_ instance, you will see some _RBAC_ policies starting like *g*  and *p*:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
cat cluster-addons/charts/bootstrap/templates/argocd/argocd.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
...
  rbac:
    defaultPolicy: ''
    policy: |-
      g, {{ $.Values.argocd.group }}, role:admin
      p, role:operator, applications, get, */*, allow
      p, role:operator, applications, sync, */*, allow
      g, argo-admins, role:admin 
      g, argo-readers, role:readonly
      g, argo-operators, role:operator
      g, argo-dev-operators, role:operator-dev      
    scopes: '[groups]'
...
----

Policies starting with _'g'_ assign roles to OpenShift local groups (they can be both Argo roles and OpenShift roles) and their users. While policies starting with _'p'_ define specific policies for projects, resources, applications and their operations.

The following sections collect the information around _ArgoCD Roles_ and _ArgoCD permission_ in the managed clusters. It is important to understand the functionality matrix and permission that the following sections try to implement:

- *_argo-admins_*: group members have full permissions in ArgoCD to _admin_.

- *_argo-readers_*: group members have _read-only_ permissions in ArgoCD to access all information.

- *_argo-operators_*: group members have permission to manage applications (_get_ and _sync_) only in ArgoCD-

- *_argo-dev-operators_*: group members have permission to manage applications (_get_ and _sync_) only in _ArgoCD dev project_.

- *_apimanager01_*: user has no permissions to see anything in ArgoCD but has permissions to create objects in the OpenShift console.

Then on *_AppProject_* we can define restrictions like _source repo_, _destination servers_ and _resource whitelist_ allowed per project. Moreover you can define local roles for that _AppProject_.

Last but not least are *_Namespaces_*. _Namespaces_ are created as part of the bootstrap process by the _argo-infra_ instance in the _SNO_ so the operator in charge of managing apps lifecycle does not 
need to have _cluster-wide_ privileges. 

Do not forget to *push your changes to your working branch!*, then we are going to deploy the _manage-setup-a-<name>_ Application in order to _bootstrap_ the _SNO_:

NOTE: Make sure you are in the _sno-<name>-setup_ branch.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
git add .
git commit -m "bootstrap application for SNO"
git push origin sno-<name>-setup
----

- Now login to _argocd_ instance in _Argo Hub_ (https://argocd-server-openshift-operators.apps.argo-hub.<domain>.opentlc.com) 

- Click *_LOG IN VIA OPENSHIFT_*.

.Log in argocd console instance
image::argo-login-1.png[]

- Hit *_my_htpasswd_provider_*. 

.OpenShift credentials for argocd instance
image::hub-login-1.png[]

- Insert _userXX_ and _<pass>_ provided by instructor and click *_Log in_* button.

.argocd instance log in
image::hub-login-2.png[]

To create bootstrap application, once logged in, click on:

- *_New app_* -> *_Edit as Yaml_* 

IMPORTANT: *Insert the following Application with the changes we previously pushed to the repository*:

Update from _global-config/bootstrap-a/managed-setup.yaml_:

- *<name> -> Assigned cluster*
- *<your_user> -> Your GitHub user account*

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
vi global-config/bootstrap-a/managed-setup.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: managed-setup-a-<name>
  namespace: openshift-operators
spec:
  destination:
    namespace: openshift-operators
    server: https://kubernetes.default.svc
  project: project-sno-<name>
  source:
    repoURL: https://github.com/<your_user>/workshop-gitops-content-deploy.git
    targetRevision: sno-<name>-setup
    path: cluster-addons/cluster-addons-as/
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
----

- Then hit *_Save_* -> *_Create_*

At this point you should see some Applications on _Syncing_ status on your _argocd_ instance console. You cannot see your colleagues deployments thanks to _RBAC_.

.SNO bootstrap
image::managed-setup.png[]

Deep dive on *_managed-setup-a-<name>_* Application to check all the resources created. Next go back to the initial view and see how the Applications rendered by ApplicationSet are created.

.ApplicationSet view
image::managed-setup-a-name.png[]

Verify in _Argo Hub_ console (E.g. https://console-openshift-console.apps.argo-hub.<domain>.opentlc.com/) using your user with view role.

- *_userXX_*
- *_<pass>_* provided by instructor at the beginning of the workshop

Navigate to *_argocd_* instance:

- *_Installed Operators_* -> *_OpenShift GitOps*_ -> *_ArgoCD_* -> *_argocd_*

.argocd view
image::argocd-instance-view-1.png[]

Take a look to global _RBAC_ policies and then navigate to *_AppProject_*
to verify your local permissions.

If you try to deploy a new Application from the OpenShift _Argo Hub_ console you will see you can not deploy to a different cluster destination than your _SNO_.

Check this in the _argocd_ instance *_Settings_* section  (E.g. https://argocd-server-openshift-operators.apps.argo-hub.<domain>.opentlc.com/settings)

.Settings section
image::settings-section-argocd.png[]

- Click *_Clusters_* to view your destination cluster:

.argocd clusters
image::clusters-list.png[]

- Click *_Projects_*, it happens the same with projects, you can only see yours: 

.argocd projects
image::projects-list.png[]

Once this is completed login to you managed cluster, _SNO_, and verify:

IMPORTANT: Remember to use your <name> and <domain>.

- Open the OpenShift _SNO_ console (E.g. https://console-openshift-console.apps.sno-<name>.<domain>.opentlc.com)

- Hit *_my_htpasswd_provider_*.

.SNO log in
image::hub-login-1.png[]

- Insert _admin_ as user and _<pass>_ provided by instructor and click *_Log in_* button.

.Insert SNO crentials to log in
image::hub-login-2.png[]

Verify that:

- 1) *_OpenShift GitOps_* operator is installed.

- 2) *_argo-infra_* instance exists as _ArgoCD_ object and is _cluster-wide_.

Now Log in _argocd-infra_ instance using OpenShift credentials, with user _admin_ and <pass> provided by intructor.

- 3) Click *_Settings_* section  (E.g. https://argocd-infra-server-openshift-gitops.apps.sno-<name>.<domain>.opentlc.com/settings)

.Settings section
image::settings-section-argocd.png[]

- 4) Click *_Clusters_* to view your destination cluster, then click again in the destination cluster to view the _Namespace_ configuration, you should see _'All namespaces'_:

.General overview argocd-infra instance
image::cluster-wide.png[]

- 5) Follow the same steps to verify that *_argocd-apps_* (E.g. https://argocd-apps-server-openshift-operators.apps.sno-<name>.<domain>.opentlc.com/settings) instance exists and is NOT _cluster-wide_.

.General overview argocd-apps instance
image::argocd-apps-general-1.png[]

- 6) Both _'Dev'_ and _'Pro'_ _App Project_ exist on _argocd-apps_ instance.

Click on Projects on the left menu:

image::projects.png[]

Verify you can create apliations on _'Dev'_ project for _argocd-apps_ instance.

IMPORTANT: This app should be deployed as *prerequisite* of *_4.5 Vault Configuration subsection_*.

Make sure you push content to your working branch:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/helm-infra-gitops-workshop
git add .
git commit -m "initial commit for testing app"
git push origin sno-<name>
----

Click -> *_New app_* -> *_Edit as Yaml_* 

IMPORTANT: *Insert the following Application with the changing _<name>_ and <your_user>*:

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  namespace: openshift-operators
  name: sno-<name>-vault
spec:
  destination:
    namespace: vault-secrets
    server: 'https://kubernetes.default.svc'
  source:
    helm:
      parameters:
        - name: vault.enabled
          value: 'true'
    path: .
    repoURL: 'https://github.com/<your_user>/helm-infra-gitops-workshop.git'
    targetRevision: sno-<name>
  project: dev
  syncPolicy:
    automated:
      prune: false
      selfHeal: false  
----

Then hit *_Save_* -> *_Create_*

First log in the _SNO_, replace *_<name>_*, *_<pass>_* and *_<domain>_*:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
oc login -u admin -p <pass> https://api.sno-<name>.<domain>.opentlc.com:6443
----

When the Application is created it deploys an app called _vault_ in namespace _vault-secrets_ that will encode a password and store it in _Vault_. If you access the application route you will not be able to see the password in plain text:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
oc get route vault -n vault-secrets 
curl vault-vault-secrets.apps.sno-<name>.<domain>.opentlc.com
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
The password value is: <password | base64encode>
----

Now that we have an application successfully deployed in the _argocd-apps_ instance, it is possible to check _RBAC_ configuration, for example try:

- Login in the _argocd-apps_ instance as user user04 and <pass> (argo-dev-operators) with role _operator-dev_ and verify you can _'get'_ and _'sync'_ apps on _'dev'_ project.

- Login the _argocd-apps_ instance as user apimanager01 and <pass> (api-manager) and verify you do not have permissions to see apps on _'dev'_ project.